{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_grammar = \"\"\"\n",
    "sentence => noun phrase verb phrase\n",
    "noun_phrase => Article Adj* noun\n",
    "Adj* => null | Adj Adj*\n",
    "verb_phrase => verb noun_phrase\n",
    "Artical => 一个 | 这个\n",
    "noun => 女人 | 篮球 | 桌子 | 小猫\n",
    "verb => 看着 | 坐在 | 听着 | 看见\n",
    "Adj => 蓝色的 | 好看的 | 小小的\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiny blue'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def adj(): return random.choice((\"blue | beautiful | tiny\").split(\"|\")).split()[0]\n",
    "#     adj_collection = \"blue | beautiful | tiny\"\n",
    "#     single_adj = adj_collection.split('|')\n",
    "#     return random.choice(single_adj)\n",
    "    \n",
    "# adj()\n",
    "def adj_star(): return random.choice([None, adj() + \" \"+ adj()])\n",
    "adj_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['蓝色的'], ['好看的'], ['小小的']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_grammar = \"\"\"\n",
    "Artical => 一个 | 这个\n",
    "noun => 女人 | 篮球 | 桌子 | 小猫\n",
    "verb => 看着 | 坐在 | 听着 | 看见\n",
    "Adj => 蓝色的 | 好看的 | 小小的\"\"\"\n",
    "\n",
    "grammar = {}\n",
    "\n",
    "for line in simple_grammar.split(\"\\n\"):\n",
    "    if not line.strip(): continue\n",
    "        \n",
    "    exp, stmt = line.split(\"=>\")\n",
    "    grammar[exp] = [s.split() for s in stmt.split(\"|\")]\n",
    "\n",
    "grammar['Adj ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'小小的蓝色的null'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Adj_grammar = \"\"\"\n",
    "Adj* => null | Adj Adj*\n",
    "Adj => 蓝色的 | 好看的 | 小小的\n",
    "\"\"\"\n",
    "\n",
    "grammar = {}\n",
    "\n",
    "for line in simple_grammar.split(\"\\n\"):\n",
    "    if not line.strip(): continue\n",
    "        \n",
    "    exp, stmt = line.split(\"=>\")\n",
    "    grammar[exp.strip()] = [s.split() for s in stmt.split(\"|\")]\n",
    "\n",
    "# grammar[\"Adj*\"]\n",
    "# def generate(gram, target):\n",
    "#     if target in gram:\n",
    "#         new_expanded = random.choice(gram[target])\n",
    "#         return ''.join(generate(gram, t) for t in new_expanded)\n",
    "#     else:\n",
    "#         return target\n",
    "    \n",
    "# generate(gram = grammar, target = \"Adj*\")\n",
    "\n",
    "def generate(gram, target):\n",
    "    if target not in gram: return target\n",
    "\n",
    "    return \"\".join(generate(gram, t) for t in random.choice(gram[target]))\n",
    "\n",
    "# generate(gram = grammar, target = \"Yo\")\n",
    "generate(gram = grammar, target = \"Adj*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一个女人看见这个好看的篮球'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "simple_grammar = \"\"\"\n",
    "sentence => noun_phrase verb_phrase\n",
    "noun_phrase => Article Adj* noun\n",
    "Adj* => null | Adj Adj*\n",
    "verb_phrase => verb noun_phrase\n",
    "Article => 一个 | 这个\n",
    "noun => 女人 | 篮球 | 桌子 | 小猫\n",
    "verb => 看着 | 坐在 | 听着 | 看见\n",
    "Adj => 蓝色的 | 好看的 | 小小的\"\"\"\n",
    "\n",
    "\n",
    "def create_grammar(grammar_str):\n",
    "    grammar = {}\n",
    "    \n",
    "    for line in grammar_str.split('\\n'):\n",
    "#         \"\"\"split the string by lines\"\"\"\n",
    "        if not line.strip(): continue    \n",
    "#         \"\"\"delete the first and last line without content\"\"\"\n",
    "        exp, stmt = line.split('=>')\n",
    "#     \"\"\"divide the dictionary with key and value, key as exp, and value as stmt\"\"\"\n",
    "        grammar[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "#     \"\"\"get rid of the space in the key, and split the value with the symbal and put all the values in a list\"\"\"\n",
    "    return grammar\n",
    "\n",
    "grammar = create_grammar(grammar_str = simple_grammar)\n",
    "\n",
    "grammar\n",
    "choice = random.choice\n",
    "\n",
    "def generate(gram, target):\n",
    "    if target not in gram: return target\n",
    "\n",
    "    expaned = [generate(gram, t) for t in choice(gram[target])]\n",
    "    return \"\".join([e if e != '\\n' else '/n' for e in expaned if e != \"null\"])\n",
    "\n",
    "example_grammar = creat_grammar(simple_grammar)\n",
    "generate(gram= grammar, target = 'sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "while(da4){/n...if(b2){/n...while(ad1){/n...bb1=a3}}}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "programming = \"\"\"\n",
    "stmt => if_exp | while_exp | assignment\n",
    "assignment => var = var\n",
    "if_exp => if ( var ) { /n ... stmt }\n",
    "while_exp => while ( var ) { /n ... stmt }\n",
    "var => chars number\n",
    "chars => char | char char\n",
    "char => a | b | c | d | e\n",
    "number => 1 | 2 | 3 | 4   \n",
    "\"\"\"\n",
    "\n",
    "def create_grammar(grammar_str):\n",
    "    grammar = {}\n",
    "    \n",
    "    for line in grammar_str.split('\\n'):\n",
    "#         \"\"\"split the string by lines\"\"\"\n",
    "        if not line.strip(): continue    \n",
    "#         \"\"\"delete the first and last line without content\"\"\"\n",
    "        exp, stmt = line.split('=>')\n",
    "#     \"\"\"divide the dictionary with key and value, key as exp, and value as stmt\"\"\"\n",
    "        grammar[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "#     \"\"\"get rid of the space in the key, and split the value with the symbal and put all the values in a list\"\"\"\n",
    "    return grammar\n",
    "\n",
    "def generate(gram, target):\n",
    "    if target not in gram: return target\n",
    "\n",
    "    expaned = [generate(gram, t) for t in choice(gram[target])]\n",
    "    return \"\".join([e if e != '\\n' else '/n' for e in expaned if e != \"null\"])\n",
    "\n",
    "print(generate(gram=create_grammar(programming), target = \"stmt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Automata\n",
    "\n",
    "# language model can be viewed as the probability of the co-exitence of the string\n",
    "# P(W1W2W3W4) = P(W1|W2W3W4)*P(W2|W3W3)*P(W3|W4)\n",
    "# => p(w1w2w3w4) = p(w1|w2) p(w2|w3) p(w3|w4) p(w4)\n",
    "\n",
    "# !pip install jieba => to install jieba directly without existing the jb and go to run the lines in the terminal\n",
    "\n",
    "# import random\n",
    "# import pandas as pd\n",
    "# import re => regular expression\n",
    "# import jieba\n",
    "# from collections import Counter\n",
    "\n",
    "\n",
    "# filename to import csv file \n",
    "# content = pd.read_csv(filename, encoding = 'gb18030') => to import  file content and to encode the content\n",
    "\n",
    "#content.head() => to view the content \n",
    "\n",
    "# articles = content['content'].tolist()  => to put the content of the csv file in list\n",
    "# len(articles) => to count the length of the list, how many items in the list\n",
    "# articles[100] => to read the 100th item content of the list\n",
    "\n",
    "# def token(string):\n",
    "# return re.findall('\\w+', string) => to get rid of all invalides like the /r and /n\n",
    "\n",
    "\n",
    "# afticles_clean = [\"\".join(token(str(a) for a in articles))] => to join all the text without any invalide in one string\n",
    "# articles_cleas[0] => to see the first in the list\n",
    "\n",
    "# With_jieba_cut = Counter(jieba.cut(articles_clean)) => to cut the articles\n",
    "# with_jieba_cut.most_common(100) => to output the top 100 frequencies words\n",
    "\n",
    "#  def cut(string): return list(jieba.cut(string)) => define a function to jieba cut the string into words and return a list\n",
    "\n",
    "# articles_words = [\n",
    "#  cut(string) for string in articles_clean\n",
    "# ]\n",
    "\n",
    "\n",
    "# from functools import reduce => reduce is to reduce the sequence into one value through a function given\n",
    "# from operator import add, mul\n",
    "# reduce(add,[1,2,3,4,5] => output: 15\n",
    "\n",
    "# tokens = reduce(add, articles_words) => to add up all the words\n",
    "\n",
    "# article_num = 10000 => change it into -1 for homework, meaning till the last one\n",
    "\n",
    "# with open('article_9k.txt','w') as f:\n",
    "# for a in articles_clean:\n",
    "#     f.write(a)\n",
    "\n",
    "\n",
    "#  to write the content into a new doc\n",
    "\n",
    "\n",
    "#  for i, line in enumerate(open('articla_9k.txt').readlin())\n",
    "#  if i % 10 == 0: print(i)\n",
    "# if i > 1000: break\n",
    "# token += cut(line)\n",
    "\n",
    "#  to print the number every 10 lines\n",
    "\n",
    "\n",
    "\n",
    "# len(TOKEN)\n",
    "# from collections import Counter\n",
    "#  word_count = Counter(Token)\n",
    "# word_count.most_common(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 驱动器 C 中的卷是 OS\n",
      " 卷的序列号是 707E-E293\n",
      "\n",
      " C:\\Users\\Mia QIN\\Untitled Folder 3 的目录\n",
      "\n",
      "2019/07/05  10:17    <DIR>          .\n",
      "2019/07/05  10:17    <DIR>          ..\n",
      "2019/07/02  18:09    <DIR>          .ipynb_checkpoints\n",
      "2019/07/05  10:17            10,981 Lesson 1.ipynb\n",
      "               1 个文件         10,981 字节\n",
      "               3 个目录 18,010,210,304 可用字节\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for pip\n",
      "Reading https://pypi.python.org/simple/pip/\n",
      "Downloading https://files.pythonhosted.org/packages/93/ab/f86b61bef7ab14909bd7ec3cd2178feb0a1c86d451bc9bccd5a1aedcde5f/pip-19.1.1.tar.gz#sha256=44d3d7d3d30a1eb65c7e5ff1173cdf8f7467850605ac7cc3707b6064bddd0958\n",
      "Best match: pip 19.1.1\n",
      "Processing pip-19.1.1.tar.gz\n",
      "Writing C:\\Users\\MIAQIN~1\\AppData\\Local\\Temp\\easy_install-ubplvzv3\\pip-19.1.1\\setup.cfg\n",
      "Running pip-19.1.1\\setup.py -q bdist_egg --dist-dir C:\\Users\\MIAQIN~1\\AppData\\Local\\Temp\\easy_install-ubplvzv3\\pip-19.1.1\\egg-dist-tmp-4i69tpra\n",
      "creating d:\\python\\anaconda\\ancd\\lib\\site-packages\\pip-19.1.1-py3.6.egg\n",
      "Extracting pip-19.1.1-py3.6.egg to d:\\python\\anaconda\\ancd\\lib\\site-packages\n",
      "Adding pip 19.1.1 to easy-install.pth file\n",
      "Installing pip-script.py script to D:\\Python\\Anaconda\\ancd\\Scripts\n",
      "Installing pip.exe script to D:\\Python\\Anaconda\\ancd\\Scripts\n",
      "Installing pip3-script.py script to D:\\Python\\Anaconda\\ancd\\Scripts\n",
      "Installing pip3.exe script to D:\\Python\\Anaconda\\ancd\\Scripts\n",
      "Installing pip3.6-script.py script to D:\\Python\\Anaconda\\ancd\\Scripts\n",
      "Installing pip3.6.exe script to D:\\Python\\Anaconda\\ancd\\Scripts\n",
      "\n",
      "Installed d:\\python\\anaconda\\ancd\\lib\\site-packages\\pip-19.1.1-py3.6.egg\n",
      "Processing dependencies for pip\n",
      "Finished processing dependencies for pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: no files found matching 'docs\\docutils.conf'\n",
      "warning: no previously-included files found matching '.coveragerc'\n",
      "warning: no previously-included files found matching '.mailmap'\n",
      "warning: no previously-included files found matching '.appveyor.yml'\n",
      "warning: no previously-included files found matching '.travis.yml'\n",
      "warning: no previously-included files found matching 'tox.ini'\n",
      "warning: no files found matching 'Makefile' under directory 'docs'\n",
      "warning: no files found matching '*.bat' under directory 'docs'\n",
      "warning: no previously-included files found matching 'src\\pip\\_vendor\\six'\n",
      "warning: no previously-included files found matching 'src\\pip\\_vendor\\six\\moves'\n",
      "warning: no previously-included files matching '*.pyi' found under directory 'src\\pip\\_vendor'\n",
      "no previously-included directories found matching '.github'\n",
      "no previously-included directories found matching '.azure-pipelines'\n",
      "no previously-included directories found matching 'docs\\build'\n",
      "no previously-included directories found matching 'news'\n",
      "no previously-included directories found matching 'tasks'\n",
      "no previously-included directories found matching 'tests'\n",
      "no previously-included directories found matching 'tools'\n"
     ]
    }
   ],
   "source": [
    "!easy_install pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.douban.com/simple/\n",
      "Requirement already satisfied: jieba in d:\\python\\anaconda\\ancd\\lib\\site-packages (0.39)\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba -i https://pypi.douban.com/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5854680528187358e-05"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "Token = []\n",
    "for i, line in enumerate(open('article_9k.txt', 'rb')):\n",
    "#     if i % 100 == 0: \n",
    "#         print(i)\n",
    "    if i > 1000:\n",
    "        break\n",
    "    Token += jieba.cut(line)\n",
    "    \n",
    "# len(Token)\n",
    "words_count = Counter(Token)\n",
    "# print(words_count)\n",
    "# words_count.most_common(10)\n",
    "\n",
    "def prob_word(word): return words_count[word] / len(Token)\n",
    "\n",
    "# round(prob_word('你好'),6)\n",
    "\n",
    "Token = [str(t) for t in Token]\n",
    "# print(Token)\n",
    "Token_2_gram = [''.join(Token[i: i+2]) for i in range(len(Token[:-1]))]\n",
    "# Token_2_gram[:11]\n",
    "\n",
    "words_count_2 = Counter(Token_2_gram)\n",
    "def prob_2_word(word1, word2):\n",
    "    if word1 +  word2 in words_count_2:\n",
    "        return words_count_2[word1+word2] / len(Token_2_gram)\n",
    "    else:\n",
    "        return 1 / len(Token_2_gram)\n",
    "    \n",
    "prob_2_word(\"小米\", \"6\")\n",
    "\n",
    "# def get_probablity(sentence):\n",
    "#     words = cut(sentence)\n",
    "    \n",
    "#     sentence_pro = 1\n",
    "    \n",
    "#     for i, word in enumerate(words[: -1]):\n",
    "#         next_ = words[i+1]\n",
    "#         probability = prob_2_word(word, next_)\n",
    "#         sentence_pro *= probability\n",
    "        \n",
    "#     return sentence_pro\n",
    "\n",
    "# get_probablity(\"从前山里有个庙\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
